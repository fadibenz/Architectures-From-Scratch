# Machine Learning Architectures & Deep Learning Implementations

This is a meta-repository that includes all architectures / regularization techniques I implemented from scratch, it provde to be an important step in grasping the design choices, the inductive biases and backpropagation. 


## ğŸ“Œ Table of Contents
- [Multi-Layer Perceptron](#multi-layer-perceptron)
- [Optimizers and Initialization Schemes](#optimizers-and-initialization-schemes)
- [BatchNorm and Dropout](#batchnorm-and-dropout)
- [Convolutional Neural Network](#convolutional-neural-network)
- [Recurrent Neural Network](#recurrent-neural-network)
- [Autoencoder](#autoencoder)
- [Transformer](#transformer)

---

## ğŸ“ˆ Architectures & Regularization techniques:

### Multi-Layer Preceptron:

- ğŸ“‚ **Project Repo:** [Multi-Layer Preceptron](https://github.com/fadibenz/FullyConnectedNN-Vanilla-Numpy)
- ğŸ“ **Description:** A modular implementation of fully connected neural networks from scratch, featuring forward and backward propagation, various layer types, and a flexible solver for training.

---

### Optimizers and Initialization Schemes:

- ğŸ“‚ **Project Repo:** [Initialization Schemes](https://github.com/fadibenz/Optimization_Initialization_FullyConnedctedNN)
- ğŸ“ **Description:**  A modular implementation of Momentum and Adam optimizers and the different initizalization schemes (Xavier, He)

---

### BatchNorm and Dropout:
- ğŸ“‚ **Project Repo:** [BatchNorm and Dropout](https://github.com/fadibenz/BatchNorm-Dropout-VanillaNumpy)
- ğŸ“ **Description:** A modular implementation of Batch Normalization and Dropout forward and backward pass, with symbolic computing of the derivatives for backpropagation (BatchNorm proved a bit tricky to work out)

---


### Convolutional Neural Network:

- ğŸ“‚ **Project Repo:** [CNN](https://github.com/fadibenz/Convolution-SpatialBatchNorm-VanillaNumpy)
- ğŸ“ **Description:** A modular implementation of convolutional networks from scratch, focusing on forward and backward passes for convolutional layers, max pooling, and spatial batch normalization, in addition to the full derivations for backpropagation.

---

### Recurrent Neural Network:

- ğŸ“‚ **Project Repo:** [Recurrent Neural Network](https://github.com/fadibenz/RNNs-FromScratch-Gradient-Analysis)
- ğŸ“ **Description:** A modular implementation of a simple Recurrent Neural Network (RNN) layer  using PyTorch and an RNN-based regression model built on top.
--- 

### Autoencoder:

- ğŸ“‚ **Project Repo:** [Autoencoder](https://github.com/fadibenz/Autoencoders_FromScratch)
- ğŸ“ **Description:**  Implementation of three types of autoencoders (Vanilla, Denoising, Masked) using synthetic and MNIST datasets.It includes implementations, training workflows, and performance evaluations using linear probing. 
---
### Transformer:

- ğŸ“‚ **Project Repo:** [Transformer](https://github.com/fadibenz/Transformer-Summarization-)
- ğŸ“ **Description:** Full Implementation of Transformer (Scaled-dot product, Multi-head Attention, transformer layers and encoder/decoder with causal and non-causal attention)

--- 
